| 模型名称               | 参数量            | 上下文窗口 | 生成速度              | 训练成本     | 适用场景                                                                 | 关键特性                                                                                     |
|------------------------|-------------------|------------|-----------------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| ​**GPT-4.5 Turbo**     | 1.8万亿[2,5](@ref)  | 128K Token | 2-3秒/千Token         | 6,300万美元[2](@ref) | 新闻撰写、多模态创作、复杂推理                                   | 多模态输入（文本/图像/音频）、稀疏注意力优化、支持代码解释器[6,8,10](@ref)                  |
| ​**Gemini Ultra**      | 1.6万亿          | 256K Token | 3-5秒/千Token         | 未公开       | 实时数据分析、跨语言翻译                                     | 深度集成Google搜索、多语言支持（100+语言）[^未提及]                                        |
| ​**Claude 3**          | 1.5万亿[1](@ref)     | 200K Token | 5-8秒/千Token         | 未公开       | 长文档创作、教育工具开发                                     | "Artifacts"交互应用生成、低重复度参数优化[1](@ref)                                              |
| ​**GPT-4**             | 1.8万亿[2,5](@ref) | 128K Token | 2-3秒/千Token         | 6,300万美元[2](@ref) | 多模态处理、考试场景（前10%成绩）                           | 支持图像理解、数学推理能力突出[11,13](@ref)                                                  |
| ​**GPT-3.5**           | 1750亿[1](@ref)       | 8K Token   | 50 Token/秒           | 未明确       | 日常对话、基础代码生成                                       | 基于RLHF优化、API轻量级调用[1,6](@ref)                                                        |
| ​**LLaMA-3**           | 7B/70B/400B[15](@ref)| 128K Token | 70B:50 Token/秒[24](@ref)  | 未公开       | 开源社区开发、多语言场景                                     | 分组查询注意力（GQA）、支持扩展至128K窗口[16,22](@ref)                                       |
| ​**DeepSeek-R1**       | 130B[25](@ref)       | 32K Token  | 同规模快20%[26](@ref)      | 未公开       | 中文内容生成、数学题解                                       | 中文语法深度优化、推理能力增强[25,34](@ref)                                                  |
| ​**DeepSeek-V3**       | 671B（MoE）      | 128K Token | 60 Token/秒           | 558万美元    | 中文数学竞赛、多语言翻译                                     | 混合专家架构（MoE）、无辅助损失平衡[25,27](@ref)                                              |
| ​**Mistral-8x22B**     | 22B（MoE）[37](@ref) | 48K Token  | 效率高3倍[43](@ref)        | 未公开       | 欧洲多语言场景、代码生成                                     | 法语优先优化、MoE架构降低推理成本[37,43](@ref)                                                |
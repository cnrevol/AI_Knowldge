| 模型名称               | 参数量            | 上下文窗口 | 生成速度              | 训练成本     | 适用场景                                                                 | 关键特性                                                                                     |
|------------------------|-------------------|------------|-----------------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| ​**GPT-4.5 Turbo**     | 1.8万亿 | 128K Token | 2-3秒/千Token         | 6,300万美元| 新闻撰写、多模态创作、复杂推理                                   | 多模态输入（文本/图像/音频）、稀疏注意力优化、支持代码解释器                  |
| ​**Gemini Ultra**      | 1.6万亿          | 256K Token | 3-5秒/千Token         | 未公开       | 实时数据分析、跨语言翻译                                     | 深度集成Google搜索、多语言支持（100+语言）                                        |
| ​**Claude 3.5**          | 1.5万亿     | 200K Token | 5-8秒/千Token         | 未公开       | 长文档创作、教育工具开发                                     | "Artifacts"交互应用生成、低重复度参数优化                                              |
| ​**GPT-4**             | 1.8万亿 | 128K Token | 2-3秒/千Token         | 6,300万美元 | 多模态处理、考试场景（前10%成绩）                           | 支持图像理解、数学推理能力突出                                                  |
| ​**GPT-3.5**           | 1750亿       | 8K Token   | 50 Token/秒           | 未明确       | 日常对话、基础代码生成                                       | 基于RLHF优化、API轻量级调用                                                       |
| ​**LLaMA-3**           | 7B/70B/400B| 128K Token | 70B:50 Token/秒 | 未公开       | 开源社区开发、多语言场景                                     | 分组查询注意力（GQA）、支持扩展至128K窗口                                      |
| ​**DeepSeek-R1**       | 130B      | 32K Token  | 同规模快20%      | 未公开       | 中文内容生成、数学题解                                       | 中文语法深度优化、推理能力增强                                                 |
| ​**DeepSeek-V3**       | 671B（MoE）      | 128K Token | 60 Token/秒           | 558万美元    | 中文数学竞赛、多语言翻译                                     | 混合专家架构（MoE）、无辅助损失平衡                                             |
| ​**Mistral-8x22B**     | 22B（MoE） | 48K Token  | 效率高3倍        | 未公开       | 欧洲多语言场景、代码生成                                     | 法语优先优化、MoE架构降低推理成本                                                |
| 模型名称               | 参数量            | 上下文窗口 | 生成速度              | 训练成本     | 适用场景                                                                 | 关键特性                                                                                     |
|------------------------|-------------------|------------|-----------------------|--------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| ​**GPT-4.5 Turbo**     | 1.8万亿 | 128K Token | 2-3秒/千Token         | 6,300万美元| 新闻撰写、多模态创作、复杂推理                                   | 多模态输入（文本/图像/音频）、稀疏注意力优化、支持代码解释器                  |
| ​**Gemini Ultra**      | 1.6万亿          | 256K Token | 3-5秒/千Token         | 未公开       | 实时数据分析、跨语言翻译                                     | 深度集成Google搜索、多语言支持（100+语言）                                        |
| ​**Claude 3.5**          | 1.5万亿     | 200K Token | 5-8秒/千Token         | 未公开       | 长文档创作、教育工具开发                                     | "Artifacts"交互应用生成、低重复度参数优化                                              |
| ​**GPT-4**             | 1.8万亿 | 128K Token | 2-3秒/千Token         | 6,300万美元 | 多模态处理、考试场景（前10%成绩）                           | 支持图像理解、数学推理能力突出                                                  |
| ​**GPT-3.5**           | 1750亿       | 8K Token   | 50 Token/秒           | 未明确       | 日常对话、基础代码生成                                       | 基于RLHF优化、API轻量级调用                                                       |
| ​**LLaMA-3**           | 7B/70B/400B| 128K Token | 70B:50 Token/秒 | 未公开       | 开源社区开发、多语言场景                                     | 分组查询注意力（GQA）、支持扩展至128K窗口                                      |
| ​**DeepSeek-R1**       | 130B      | 32K Token  | 同规模快20%      | 未公开       | 中文内容生成、数学题解                                       | 中文语法深度优化、推理能力增强                                                 |
| ​**DeepSeek-V3**       | 671B（MoE）      | 128K Token | 60 Token/秒           | 558万美元    | 中文数学竞赛、多语言翻译                                     | 混合专家架构（MoE）、无辅助损失平衡                                             |
| ​**Mistral-8x22B**     | 22B（MoE） | 48K Token  | 效率高3倍        | 未公开       | 欧洲多语言场景、代码生成                                     | 法语优先优化、MoE架构降低推理成本                                                |
| ​**QwQ-32B**​（阿里）   | 32B               | 32K-131K     | 单卡RTX4090可部署，|成本低| 数学推理、编程开发、工具调用                                             | • 强化学习优化推理能力，LiveBench数学/代码评分全球前五<br>• 支持结构化自我提问机制和工具调用|
| ​**Doubao-1.5 Pro**​（字节） | 未公开（稀疏MoE） | 支持256K长文本处理       | ¥0.0008/千Token输入       |未公开 | 多模态处理、语音交互、低成本大规模应用                                   | • 中文理解/编程能力超越GPT-4o<br>• 支持端到端语音对话，284张720P图片处理仅需¥1              |
| ​**Qwen1.5-72B**       | 72B（Base/Chat）           | 32K-131K    | 单卡RTX4090部署[1](@ref)          | 未公开       | 数学推理、代码生成、多语言处理                                 | • 支持12种语言<br>• 量化模型支持Int4/Int8[1](@ref)<br>• 检索增强生成（RAG）优化[1](@ref)              |
| ​**Qwen1.5-MoE-A2.7B** | 27亿激活参数（等效7B密集） | 32K         | 推理速度↑1.74倍[1](@ref)          | 降低75%[1](@ref)  | 轻量级推理、边缘计算                                           | • 细粒度专家分割（64个expert）<br>• 混合共享/路由专家机制[1](@ref)                               |
| ​**Doubao-1.5Pro**      | 等效7×密集模型[6,8](@ref)     | 未明确      | ¥0.0008/千Token输入[7](@ref)      | 降低90%[8](@ref)  | 多模态交互、企业级复杂任务                                     | • 稀疏MoE架构<br>• 支持端到端语音对话[6](@ref)<br>• 视觉理解全球领先[8](@ref)                         |
| ​**Doubao通用Pro**      | 未公开                     | 256K        | 日均处理1200亿Token[用户数据]| 自研集群优化[8](@ref) | 商用长文本处理、大规模内容生成                                 | • 分布式推理优化<br>• 支持3000万张图片生成/日[用户数据]                                      |
| ​**Doubao通用Lite**     | 未公开                     | 未明确      | 成本↓84% / 延迟↓50%[用户数据]| 低成本芯片支持[8](@ref) | 中小企业经济型场景                                             | • 精细量化技术<br>• 多任务混合调度[8](@ref)                                                      |